# Implementing Skipgram
This repository implements the skipgram language model from [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf). The model is trained on the top 30 most downloaded books on Project Gutenberg and evaluated first on both in-distribution accuracy and loss. The word embeddings generated by the model are also evaluated on a downstream task of completing analogies. 

The vast majority of this code is copied from the homework repository. I implemented only the creation of training and validation datasets from the encoded corpus, as well as the skipgram model itself. 

## Reproducing these results
To train the model yourself, you'll need to clone the repo and install the packages listed in requirements.txt. Then you can begin training with this command:
```
python3 train.py \
--analogies_fn analogies_v3000_1309.json \
--books_dir books/ \
--num_epochs 5 \
--val_every 1
```

## Architecture
### Model
Skipgram is a single-layer neural language model proposed to efficiently learn word embeddings. It predicts a target token by using the words immediately preceding and following it within a fixed window size. (Here, we look 2 tokens forwards and back.) These context tokens are embedded into vectors of 128 dimensions and then summed to produce a single vector. A fully connected layer projects the hidden vector to the size of the vocabulary, and a softmax layer provides probabilities for each output token. 

### Data
The homework repository provides code for downloading text files from Project Guterberg, extracting sentences with the spacy library's sentencizer, preprocessing them to remove non-alphanumeric characters, and tokenizing the sentences with a learned vocabulary. 

To generate training and validation data for the model, I set each token as an input to the model and created a label for it. The labels are multi-hot vectors encoding the four context tokens -- two preceding the target token, and two following it. I split the data into training and validation sets randomly, using 20% of tokens as validation data. 

### Evaluation Metrics
The model is trained on binary cross entropy, or BCE. Unlike the more common categorical cross entropy, BCE allows each input to have multiple correct outputs, as indicated by the multi-hot vector expressing the four context tokens. The model is graded on the probability assigned to each of those four tokens, achieving lower loss when more probability mass is on those correct answers. Formally, BCE can be expressed as: 

For a more intuitive in-distribution evaluation metric, we use intersection over union (IOU). This takes the set of four tokens with the highest probability according to the model, calculates the cardinalities of their intersection and union with the set of context tokens, and reports the ratio between the two. This provides a metric scaled between zero and one where higher scores are better and incorrect guesses are penalized.

After training is complete, the learned word embeddings are evaluated on completing semantic and syntactic analogies. Analogies of the form "A is to B as C is to D" are provided by the homework repo. To evaluate the word embeddings, the difference in word embeddings between A and B is added to the word embedding of C, and the word embeddings most similar to that result are retrieved. Three metrics are reported: 
* __Exact__: The percentage of analogies where the embedding of D is the most similar of all word embeddings to the result of C + B - A. 
* __Mean Reciprocal Rank__: Averaged across all analogies, the reciprocals of the similarity rankings of D to C + B - A. 
* __Mean Rank__: The average similarity ranking of D to C + B - A. 

## Performance
Here are the in-distribution performance charts. In both training and validation, loss falls steadily while intersection over union performance increases over the epochs. There is some evidence of overfitting, with validation loss occasionally higher than training loss. 

TODO: Insert chart here. 

On the downstream analogies task, performance is stronger on syntactic analogies than on semantic analogies. This could be a result of a small context window providing little information about the sentence except for parts of speech. Encouragingly, the model achieves 25% top-1 accuracy on 12 semantic analogies about gender, such as "man is to woman as king is to queen." 

Here is the performance chart for semantic analogies. 
| relation         |N       |exact   |MRR     |MR|
| --- | --- |  --- |  --- | --- | 
| all analogies    | 1309 | 0.0237 | 0.0439 | 23 |
| capitals         | 1       | 0.0000  | 0.0200  | 50|
| binary_gender    | 12      | 0.2500  | 0.3997  | 3|
| antonym          | 54      | 0.0185  | 0.0507  | 20|
| member           | 4       | 0.0000  | 0.0151  | 66|
| hypernomy        | 542     | 0.0018  | 0.0102  | 98|
| similar          | 117     | 0.0085  | 0.0225  | 44|
| partof           | 29      | 0.0000  | 0.0122  | 82|
| instanceof       | 9       | 0.0000  | 0.0056  | 180|
| derivedfrom      | 133     | 0.0000  | 0.0050  | 199|
| hascontext       | 32      | 0.0000  | 0.0146  | 68|
| relatedto        | 10      | 0.0000  | 0.0017  | 579|
| attributeof      | 11      | 0.0909  | 0.1018  | 10|
| causes           | 6       | 0.0000  | 0.0017  | 580|
| entails          | 9       | 0.0000  | 0.0117  | 85|


And here is the performance chart for syntactic analogies. 
| relation | N | exact | MRR  | MR | 
| --- | --- |  --- |  --- | --- | 
| all analogies    | 340 | 0.0706 | 0.1145 | 9 |
| adj_adv         | 22      | 0.0000  | 0.0058  | 174 | 
| comparative     | 7       | 0.2857  | 0.3371  | 3 | 
| superlative     | 3       | 0.3333  | 0.4000  | 2 |
| pres_particip   | 62      | 0.0806  | 0.1115  | 9 |
| denonym         | 2       | 0.5000  | 0.5072  | 2 |
| past_tense      | 64      | 0.0938  | 0.1498  | 7 |
| plural_nouns    | 107     | 0.0374  | 0.0912  | 11 |
| plural_verbs    | 73      | 0.0685  | 0.1091  | 9 |